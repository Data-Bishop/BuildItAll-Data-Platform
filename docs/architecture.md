# **Architecture Documentation**

## **1. Purpose**
The purpose of this architecture is to build a **data platform** that supports:
- **Synthetic Data Generation**: Using a Spark job to generate synthetic parquet datasets.
- **Data Processing**: Processing raw generated parquet datasets using Spark on Amazon EMR.
- **Orchestration**: Managing workflows and scheduling tasks using Apache Airflow.
- **Storage**: Storing raw, processed, orchestration-related, logs and other data in Amazon S3.
- **Networking**: Ensuring secure communication between components using a VPC with public and private subnets.
- **Access Control**: Using IAM roles and policies to enforce least-privilege access to AWS resources.

---

## **2. Content**

### **Compute: EMR**
- **Purpose**: Amazon EMR is used to run Spark jobs for data generation and processing.
- **Configuration**:
  - **Cluster**: Configured with one master node (`m5.xlarge`) and one core node (`m5.xlarge`).
  - **Applications**: Spark is installed on the cluster.
  - **Bootstrap Actions**: A bootstrap script (`bootstrap.sh`) installs spark job dependencies like Python libraries (e.g `faker`).
  - **Security Groups**:
    - Master and slave nodes have security groups allowing internal communication and SSH access.
  - **IAM Roles**:
    - `EMR_DefaultRole`: Allows the cluster to interact with AWS services.
    - `EMR_EC2_DefaultRole`: Grants EC2 instances in the cluster access to S3 and other resources.

---

### **Orchestration: Airflow**
- **Purpose**: Apache Airflow is used to orchestrate workflows for data generation and processing.
- **Configuration**:
  - **Deployment**: Airflow is deployed on an EC2 instance using Docker Compose.
  - **Components**:
    - **Webserver**: Accessible on port `8080`.
    - **Scheduler**: Manages task execution.
    - **Worker**: Executes tasks in parallel.
    - **Postgres**: Used as the metadata database.
    - **Redis**: Used as the Celery broker.
  - **DAGs**:
    - `data_generation_dag.py`: Generates synthetic parquet datasets using Spark on EMR.
    - `data_processing_dag.py`: Processes raw parquet datasets using Spark on EMR.
  - **Security**:
    - Airflow's EC2 instance is in a private subnet.
    - Port forwarding or SSM is used to access the Airflow UI securely.

---

### **Storage: S3**
- **Purpose**: Amazon S3 is used to store raw, processed, and orchestration-related data.
- **Buckets**:
  - `builditall-client-data`:
    - **Folders**:
      - `raw/`: Stores raw data generated by Spark jobs.
      - `processed/`: Stores processed data.
      - `scripts/`: Stores Spark job scripts and other scripts.
  - `builditall-airflow`:
    - Dockerfile, docker-compose file and airflow setup scripts are stored in the root of this bucket.
    - **Folders**:
      - `dags/`: Stores Airflow DAGs, email notification and utility scripts.
      - `requirements/`: Stores Python dependencies for Airflow.
  - `builditall-logs`:
    - **Folders**:
      - `airflow/`: Stores Airflow logs.
      - `emr/`: Stores EMR logs.
  - `builditall-tfstate`:
    - Stores Terraform backend and state configuration.
- **Bucket Policies**:
  - Grant access to specific IAM roles (e.g., Airflow and EMR roles) for reading and writing data.

---

### **Networking**
- **VPC**:
  - A custom VPC is created with the following:
    - **Public Subnets (2)**: For the bastion host and NAT gateway.
    - **Private Subnets (2)**: For the Airflow and EMR instances.
  - **Routing**:
    - Public subnets have an internet gateway for outbound traffic.
    - Private subnets use a NAT gateway for internet access.
- **Security Groups**:
  - **Bastion Host**:
    - Allows SSH access from a specific IP range (`allowed_ip`).
  - **Airflow**:
    - Allows access to port `8080` for the Airflow UI (restricted to the VPC CIDR).
    - Allows SSH access from the bastion host.
  - **EMR**:
    - Master and slave nodes allow internal communication and SSH access.

---

### **IAM Roles**
- **Airflow Role**:
  - Grants access to:
    - S3 buckets (`builditall-airflow`, `builditall-client-data`, `builditall-logs`).
    - EMR clusters for job submission and monitoring.
- **EMR Roles**:
  - `EMR_DefaultRole`: Grants the EMR cluster access to AWS services.
  - `EMR_EC2_DefaultRole`: Grants EC2 instances in the cluster access to S3 and other resources.
- **Bastion Role**:
  - Grants access to SSM for secure session management.

---

### **Secrets Management**
- **Purpose**: AWS Secrets Manager is used to securely store sensitive variable values required by Terraform.
- **Configuration**:
  - A secret named `builditall-secrets` is created in AWS Secrets Manager.
  - The secret contains key-value pairs for sensitive variables such as:
    - `aws_region`, `project_name`, `data_bucket_name`, `airflow_bucket_name`, `logs_bucket_name`, `ami_id`, `key_pair_name`, `allowed_ip`, and `vpc_cidr`.
  - Terraform dynamically fetches these values using the `aws_secretsmanager_secret` and `aws_secretsmanager_secret_version` data sources.

---

## **3. Workflow**

### **Step 1: Data Generation**
1. **Trigger**:
   - The `data_generation_dag.py` DAG runs daily at `9:00 AM`.
2. **Workflow**:
   - Creates an EMR cluster.
   - Submits a Spark job (`data_generator.py`) to generate synthetic data.
   - Saves the generated parquet dataset to the run date subfolder (e.g `2o25-04-26`), in the `raw/` folder in the `builditall-client-data` S3 bucket.
   - Terminates the EMR cluster after the job completes.

---

### **Step 2: Data Processing**
1. **Trigger**:
   - The `data_processing_dag.py` DAG runs daily at `6:00 PM`.
2. **Workflow**:
   - Creates an EMR cluster.
   - Submits a Spark job (`data_processor.py`) to process raw parquet datasets/files generated that day from the data generation workflow.
   - Saves the processed data to the run date subfolder (e.g `2o25-04-26`), in the `processed/` folder in the `builditall-client-data` S3 bucket.
   - Terminates the EMR cluster after the job completes.

---

### **Step 3: Orchestration**
1. **Airflow**:
   - Manages the scheduling and execution of the `data_generation` and `data_processing` DAGs.
   - Sends email notifications on task success or failure using the `email_alert.py` script.

---

### **Step 4: Access**
1. **Airflow UI**:
   - Accessed via port forwarding through the bastion host or using SSM.
2. **Bastion Host**:
   - Used to SSH into private instances (e.g., Airflow).

---